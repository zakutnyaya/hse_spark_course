{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Содержание\n",
    "- [MapReduce?](#mapreduce)\n",
    "- [Mapper](#mapper)\n",
    "    - [Запускаем mapper](#testmapper)\n",
    "- [Hadoop](#hadoop)\n",
    "    - [Что такое Hadoop Streaming?](#hadoopstreaming)\n",
    "    - [Список директорий в Hadoop](#hdfs_ls)\n",
    "    - [Тестируем MapReduce на простом reducer](#dummyreducer)\n",
    "    - [Shuffling и sorting](#shuffling&sorting)\n",
    "- [Reducer](#reducer)\n",
    "    - [Запускаем reducer](#run)\n",
    "- [Запускаем mapreduce job с большими данными](#moredata)\n",
    "    - [Sort результат (`sort`)](#sortoutput)\n",
    "    - [Sort результат (в MapReduce)](#sortoutputMR)\n",
    "    - [Конфигурируем сортировку с `KeyFieldBasedComparator`](#KeyFieldBasedComparator)\n",
    "    - [Определяем конфигурацию опцией -D](#configuration_variables)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MapReduce? <a name=\"mapreduce\"></a>\n",
    "\n",
    "MapReduce - это модель распределенных параллельных вычислений разработанная для процессов обработки больших объемов данных.\n",
    "\n",
    "Данные разделяются по специальным узлам(nodes), где работают процессы - mappers. Мапперы - это первый шаг обработки данных, они делают \"базовую обработку\" и передают результаты в reducer. Уже редьюсеры объединяют данные и создают финальный результат.\n",
    "\n",
    "![Map & Reduce](mapreduce.png)\n",
    "C [Hadoop Streaming](https://hadoop.apache.org/docs/current/hadoop-streaming/HadoopStreaming.html) возможно использовать языки программирования для разработки mapper и reducer. Здесь будет описан способ использования Unix `bash`. ([Здесь](https://www.gnu.org/software/bash/manual/html_node/index.html) документация по bash).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapper <a name=\"mapper\"></a>\n",
    "\n",
    "Давайте составим первый mapper скрипт `map.sh`. Mapper должен разбивать каждую строку на слова, добавлять число для счетчика и возвращать каждое слово отдельной строй, а через tab число для счетчика - 1.\n",
    "\n",
    "Пример: input \n",
    "<html>\n",
    "<pre>\n",
    "apple orange\n",
    "banana apple peach\n",
    "</pre>\n",
    "</html>\n",
    "\n",
    "`map.sh` результат:\n",
    "<html>\n",
    "<pre>\n",
    "apple   1\n",
    "orange  1\n",
    "banana  1\n",
    "apple  1\n",
    "peach  1\n",
    "</pre>\n",
    "</html>\n",
    "\n",
    "\n",
    "<a href=\"https://ipython.readthedocs.io/en/stable/interactive/magics.html\">_cell magic_</a> [`%%writefile`](https://ipython.readthedocs.io/en/stable/interactive/magics.html#cellmagic-writefile) позволяют писать скрипты и выполнять команды Linux в Jupyter Notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ap.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile ap.txt\n",
    "\n",
    "apple orange\n",
    "banana apple peach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting map.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile map.sh\n",
    "#!/bin/bash\n",
    "\n",
    "while read line\n",
    "do\n",
    " for word in $line \n",
    " do\n",
    "  if [ -n \"$word\" ] \n",
    "  then\n",
    "     echo -e ${word}\"\\t1\"\n",
    "  fi\n",
    " done\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "apple orange\r\n",
      "banana apple peach"
     ]
    }
   ],
   "source": [
    "!cat ap.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В результате вы получите файл `map.sh` в вашей текущей директории.\n",
    "\n",
    "**Note:** Каждый последущий запуск ячейки перезапишет файл `map.sh`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rwx------  1 pro  staff   125B  2 мар 18:21 \u001b[31mmap.sh\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls -hl map.sh\n",
    "\n",
    "#dir - для Windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Запускаем mapper <a name=\"testmapper\"></a>\n",
    "\n",
    "Чтобы запустить mapper, сначала создадим для его работы данные, создадим файл `fruits.txt` с набором фруктов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting fruits.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile fruits.txt\n",
    "apple banana\n",
    "peach orange peach peach\n",
    "pineapple peach apple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple banana\r\n",
      "peach orange peach peach\r\n",
      "pineapple peach apple"
     ]
    }
   ],
   "source": [
    "!cat fruits.txt\n",
    "\n",
    "# type - в Windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выполним mapper. Используем для этого pipeline '|'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple\t1\r\n",
      "banana\t1\r\n",
      "peach\t1\r\n",
      "orange\t1\r\n",
      "peach\t1\r\n",
      "peach\t1\r\n"
     ]
    }
   ],
   "source": [
    "# получили ключ-значение\n",
    "# мы сначала выводим все на экран, так как между мапером и редьюсером должно быть\n",
    "# промежуточное звено, которое хранит все данные. В гашем случае это - экран\n",
    "!cat fruits.txt | ./map.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       6      12      50\r\n"
     ]
    }
   ],
   "source": [
    "# добавили аггрегатор - wc (а-ля редьюсер)\n",
    "!cat fruits.txt | ./map.sh | wc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если скрипт `map.sh` не выполнится, то проверте права на его использование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod 700 map.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadoop <a name=\"hadoop\"></a>\n",
    "Теперь воспользуемся Hadoop и запустим наш скрипт с помощью Hadoop Streaming. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Что такое Hadoop Streaming <a name=\"hadoopstreaming\"></a>\n",
    "\n",
    "Hadoop Streaming - это библиотека в Hadoop, которая разработка для созданиях самописных мапперов и редьюсеров в исполняемые процессы MapReduce. \n",
    "\n",
    "\n",
    "Mapper и reducer читают данные из stdin и отправляют их в stdout. Обучно, колонки в данных разделяются с помощью `tab`. Если данные разделены другим разделителем, то надо будет определять разделитель. Для этого ознакомьтесь с `TextInputFormat` (see the [API documentation](https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/TextInputFormat.html)) и [Hadoop Streaming documentation](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Customizing_How_Lines_are_Split_into_KeyValue_Pairs).\n",
    "\n",
    "Пример MapReduce streaming синтаксиса:\n",
    "<html>\n",
    "<pre>\n",
    "    mapred streaming \\\n",
    "  -input myInputDirs \\\n",
    "  -output myOutputDir \\\n",
    "  -mapper /bin/cat \\\n",
    "  -reducer /usr/bin/wc\n",
    "\n",
    "</pre>\n",
    "</html>\n",
    "\n",
    "Документация для Hadoop Streaming от Apache Hadoop: [https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html).\n",
    "\n",
    "Все настройки Hadoop Streaming и опции описаны здесь: [Streaming Command Options](https://hadoop.apache.org/docs/current/hadoop-streaming/HadoopStreaming.html#Streaming_Command_Options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: mapred: command not found\r\n"
     ]
    }
   ],
   "source": [
    "!mapred streaming --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Список директорий в Hadoop <a name=\"hdfs_ls\"></a>\n",
    "\n",
    "Команда `hdfs dfs -l` покажет вам все, что находится в вашей домашней директории HDFS. \n",
    "\n",
    "`hdfs dfs` запускает файловую систему Hadoop. Списко всех достуных команды вы найдете в документации [System Shell Guide](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html#dfs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PYTHON(1)                                                            PYTHON(1)\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "N\bNA\bAM\bME\bE\r\n",
      "       python  - an interpreted, interactive, object-oriented programming lan-\r\n",
      "       guage\r\n",
      "\r\n",
      "S\bSY\bYN\bNO\bOP\bPS\bSI\bIS\bS\r\n",
      "       p\bpy\byt\bth\bho\bon\bn [ -\b-B\bB ] [ -\b-d\bd ] [ -\b-E\bE ] [ -\b-h\bh ] [ -\b-i\bi ] [ -\b-m\bm _\bm_\bo_\bd_\bu_\bl_\be_\b-_\bn_\ba_\bm_\be ]\r\n",
      "              [ -\b-O\bO ] [ -\b-O\bOO\bO ] [ -\b-R\bR ] [ -\b-Q\bQ _\ba_\br_\bg_\bu_\bm_\be_\bn_\bt ] [ -\b-s\bs ] [ -\b-S\bS ] [ -\b-t\bt ] [  -\b-u\bu\r\n",
      "       ]\r\n",
      "              [ -\b-v\bv ] [ -\b-V\bV ] [ -\b-W\bW _\ba_\br_\bg_\bu_\bm_\be_\bn_\bt ] [ -\b-x\bx ] [ -\b-3\b3 ] [ -\b-?\b?  ]\r\n",
      "              [ -\b-c\bc _\bc_\bo_\bm_\bm_\ba_\bn_\bd | _\bs_\bc_\br_\bi_\bp_\bt | - ] [ _\ba_\br_\bg_\bu_\bm_\be_\bn_\bt_\bs ]\r\n",
      "\r\n",
      "D\bDE\bES\bSC\bCR\bRI\bIP\bPT\bTI\bIO\bON\bN\r\n",
      "       Python is an interpreted, interactive, object-oriented programming lan-\r\n",
      "       guage that combines remarkable power with very clear  syntax.   For  an\r\n",
      "       introduction  to  programming  in Python, see the Python Tutorial.  The\r\n",
      "       Python Library Reference documents built-in and  standard  types,  con-\r\n",
      "       stants,  functions  and  modules.  Finally, the Python Reference Manual\r\n",
      "       describes the syntax and semantics of the  core  language  in  (perhaps\r\n",
      "       too)  much  detail.   (These  documents may be located via the I\bIN\bNT\bTE\bER\bRN\bNE\bET\bT\r\n",
      "       R\bRE\bES\bSO\bOU\bUR\bRC\bCE\bES\bS below; they may be installed on your system as well.)\r\n",
      "\r\n",
      "       Python's basic power can be extended with your own modules written in C\r\n",
      "       or  C++.   On  most  systems  such  modules  may be dynamically loaded.\r\n",
      "       Python is also adaptable as an extension language for existing applica-\r\n",
      "       tions.  See the internal documentation for hints.\r\n",
      "\r\n",
      "       Documentation  for  installed Python modules and packages can be viewed\r\n",
      "       by running the p\bpy\byd\bdo\boc\bc program.\r\n",
      "\r\n",
      "C\bCO\bOM\bMM\bMA\bAN\bND\bD L\bLI\bIN\bNE\bE O\bOP\bPT\bTI\bIO\bON\bNS\bS\r\n",
      "       -\b-B\bB     Don't write _\b._\bp_\by_\b[_\bc_\bo_\b] files on import. See  also  PYTHONDONTWRITE-\r\n",
      "              BYTECODE.\r\n",
      "\r\n",
      "       -\b-c\bc _\bc_\bo_\bm_\bm_\ba_\bn_\bd\r\n",
      "              Specify  the command to execute (see next section).  This termi-\r\n",
      "              nates the option list (following options are passed as arguments\r\n",
      "              to the command).\r\n",
      "\r\n",
      "       -\b-d\bd     Turn  on parser debugging output (for wizards only, depending on\r\n",
      "              compilation options).\r\n",
      "\r\n",
      "       -\b-E\bE     Ignore environment variables like PYTHONPATH and PYTHONHOME that\r\n",
      "              modify the behavior of the interpreter.\r\n",
      "\r\n",
      "       -\b-h\bh ,\b,  -\b-?\b? ,\b,  -\b--\b-h\bhe\bel\blp\bp\r\n",
      "              Prints the usage for the interpreter executable and exits.\r\n",
      "\r\n",
      "       -\b-i\bi     When  a  script  is passed as first argument or the -\b-c\bc option is\r\n",
      "              used, enter interactive mode after executing the script  or  the\r\n",
      "              command.  It does not read the $PYTHONSTARTUP file.  This can be\r\n",
      "              useful to inspect global variables  or  a  stack  trace  when  a\r\n",
      "              script raises an exception.\r\n",
      "\r\n",
      "       -\b-m\bm _\bm_\bo_\bd_\bu_\bl_\be_\b-_\bn_\ba_\bm_\be\r\n",
      "              Searches  _\bs_\by_\bs_\b._\bp_\ba_\bt_\bh for the named module and runs the correspond-\r\n",
      "              ing _\b._\bp_\by file as a script.\r\n",
      "\r\n",
      "       -\b-O\bO     Turn on basic optimizations.  This changes the  filename  exten-\r\n",
      "              sion  for  compiled  (bytecode)  files from _\b._\bp_\by_\bc to _\b._\bp_\by_\bo.  Given\r\n",
      "              twice, causes docstrings to be discarded.\r\n",
      "\r\n",
      "       -\b-O\bOO\bO    Discard docstrings in addition to the -\b-O\bO optimizations.\r\n",
      "\r\n",
      "       -\b-R\bR     Turn on \"hash randomization\", so that the hash() values of  str,\r\n",
      "              bytes  and  datetime  objects are \"salted\" with an unpredictable\r\n",
      "              pseudo-random value.  Although they remain  constant  within  an\r\n",
      "              individual  Python  process,  they  are  not predictable between\r\n",
      "              repeated invocations of Python.\r\n",
      "\r\n",
      "              This is intended to provide protection against a denial of  ser-\r\n",
      "              vice  caused  by  carefully-chosen inputs that exploit the worst\r\n",
      "              case performance of a dict construction, O(n^2) complexity.  See\r\n",
      "              http://www.ocert.org/advisories/ocert-2011-003.html for details.\r\n",
      "\r\n",
      "       -\b-Q\bQ _\ba_\br_\bg_\bu_\bm_\be_\bn_\bt\r\n",
      "              Division control; see PEP 238.  The  argument  must  be  one  of\r\n",
      "              \"old\"  (the  default,  int/int  and  long/long  return an int or\r\n",
      "              long), \"new\" (new division semantics, i.e. int/int and long/long\r\n",
      "              returns  a float), \"warn\" (old division semantics with a warning\r\n",
      "              for int/int and long/long), or \"warnall\" (old division semantics\r\n",
      "              with a warning for all use of the division operator).  For a use\r\n",
      "              of \"warnall\", see the Tools/scripts/fixdiv.py script.\r\n",
      "\r\n",
      "       -\b-s\bs     Don't add user site directory to sys.path.\r\n",
      "\r\n",
      "       -\b-S\bS     Disable the import of the module  _\bs_\bi_\bt_\be  and  the  site-dependent\r\n",
      "              manipulations of _\bs_\by_\bs_\b._\bp_\ba_\bt_\bh that it entails.\r\n",
      "\r\n",
      "       -\b-t\bt     Issue  a  warning  when  a source file mixes tabs and spaces for\r\n",
      "              indentation in a way that makes it depend on the worth of a  tab\r\n",
      "              expressed  in  spaces.   Issue an error when the option is given\r\n",
      "              twice.\r\n",
      "\r\n",
      "       -\b-u\bu     Force stdin, stdout and stderr to  be  totally  unbuffered.   On\r\n",
      "              systems  where  it matters, also put stdin, stdout and stderr in\r\n",
      "              binary mode.  Note that there is internal  buffering  in  xread-\r\n",
      "              lines(),  readlines()  and  file-object  iterators (\"for line in\r\n",
      "              sys.stdin\") which is not influenced by  this  option.   To  work\r\n",
      "              around  this, you will want to use \"sys.stdin.readline()\" inside\r\n",
      "              a \"while 1:\" loop.\r\n",
      "\r\n",
      "       -\b-v\bv     Print a message each time a module is initialized,  showing  the\r\n",
      "              place  (filename  or  built-in  module) from which it is loaded.\r\n",
      "              When given twice, print a message for each file that is  checked\r\n",
      "              for  when  searching for a module.  Also provides information on\r\n",
      "              module cleanup at exit.\r\n",
      "\r\n",
      "       -\b-V\bV ,\b,  -\b--\b-v\bve\ber\brs\bsi\bio\bon\bn\r\n",
      "              Prints the Python version number of the executable and exits.\r\n",
      "\r\n",
      "       -\b-W\bW _\ba_\br_\bg_\bu_\bm_\be_\bn_\bt\r\n",
      "              Warning control.  Python sometimes  prints  warning  message  to\r\n",
      "              _\bs_\by_\bs_\b._\bs_\bt_\bd_\be_\br_\br.   A  typical warning message has the following form:\r\n",
      "              _\bf_\bi_\bl_\be:\b:_\bl_\bi_\bn_\be:\b: _\bc_\ba_\bt_\be_\bg_\bo_\br_\by:\b:  _\bm_\be_\bs_\bs_\ba_\bg_\be_\b.   By  default,  each  warning  is\r\n",
      "              printed  once for each source line where it occurs.  This option\r\n",
      "              controls how often warnings are printed.   Multiple  -\b-W\bW  options\r\n",
      "              may  be  given; when a warning matches more than one option, the\r\n",
      "              action for the last matching option is  performed.   Invalid  -\b-W\bW\r\n",
      "              options  are ignored (a warning message is printed about invalid\r\n",
      "              options when the first warning is issued).  Warnings can also be\r\n",
      "              controlled  from within a Python program using the _\bw_\ba_\br_\bn_\bi_\bn_\bg_\bs mod-\r\n",
      "              ule.\r\n",
      "\r\n",
      "              The simplest form of _\ba_\br_\bg_\bu_\bm_\be_\bn_\bt is one  of  the  following  _\ba_\bc_\bt_\bi_\bo_\bn\r\n",
      "              strings  (or  a unique abbreviation): i\big\bgn\bno\bor\bre\be to ignore all warn-\r\n",
      "              ings; d\bde\bef\bfa\bau\bul\blt\bt to explicitly request the default behavior (print-\r\n",
      "              ing  each  warning once per source line); a\bal\bll\bl to print a warning\r\n",
      "              each time it occurs (this may generate many messages if a  warn-\r\n",
      "              ing  is  triggered  repeatedly for the same source line, such as\r\n",
      "              inside a loop); m\bmo\bod\bdu\bul\ble\be to print each warning only the first time\r\n",
      "              it  occurs  in  each module; o\bon\bnc\bce\be to print each warning only the\r\n",
      "              first time it occurs in the program; or e\ber\brr\bro\bor\br to raise an excep-\r\n",
      "              tion instead of printing a warning message.\r\n",
      "\r\n",
      "              The   full  form  of  _\ba_\br_\bg_\bu_\bm_\be_\bn_\bt  is  _\ba_\bc_\bt_\bi_\bo_\bn:\b:_\bm_\be_\bs_\bs_\ba_\bg_\be:\b:_\bc_\ba_\bt_\be_\bg_\bo_\br_\by:\b:_\bm_\bo_\bd_\b-\r\n",
      "              _\bu_\bl_\be:\b:_\bl_\bi_\bn_\be_\b.  Here, _\ba_\bc_\bt_\bi_\bo_\bn is as explained above but  only  applies\r\n",
      "              to messages that match the remaining fields.  Empty fields match\r\n",
      "              all values; trailing empty fields may be omitted.   The  _\bm_\be_\bs_\bs_\ba_\bg_\be\r\n",
      "              field  matches  the  start  of the warning message printed; this\r\n",
      "              match is case-insensitive.  The _\bc_\ba_\bt_\be_\bg_\bo_\br_\by field matches the warn-\r\n",
      "              ing category.  This must be a class name; the match test whether\r\n",
      "              the actual warning category of the message is a subclass of  the\r\n",
      "              specified  warning category.  The full class name must be given.\r\n",
      "              The _\bm_\bo_\bd_\bu_\bl_\be field matches the (fully-qualified) module name; this\r\n",
      "              match  is  case-sensitive.  The _\bl_\bi_\bn_\be field matches the line num-\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              ber, where zero matches all line numbers and is thus  equivalent\r\n",
      "              to an omitted line number.\r\n",
      "\r\n",
      "       -\b-x\bx     Skip  the  first line of the source.  This is intended for a DOS\r\n",
      "              specific hack only.  Warning: the line numbers in error messages\r\n",
      "              will be off by one!\r\n",
      "\r\n",
      "       -\b-3\b3     Warn  about  Python 3.x incompatibilities that 2to3 cannot triv-\r\n",
      "              ially fix.\r\n",
      "\r\n",
      "I\bIN\bNT\bTE\bER\bRP\bPR\bRE\bET\bTE\bER\bR I\bIN\bNT\bTE\bER\bRF\bFA\bAC\bCE\bE\r\n",
      "       The interpreter interface resembles that of the UNIX shell: when called\r\n",
      "       with  standard input connected to a tty device, it prompts for commands\r\n",
      "       and executes them until an EOF is read; when called with  a  file  name\r\n",
      "       argument  or  with  a  file  as standard input, it reads and executes a\r\n",
      "       _\bs_\bc_\br_\bi_\bp_\bt from that file; when called with -\b-c\bc  _\bc_\bo_\bm_\bm_\ba_\bn_\bd,  it  executes  the\r\n",
      "       Python  statement(s) given as _\bc_\bo_\bm_\bm_\ba_\bn_\bd.  Here _\bc_\bo_\bm_\bm_\ba_\bn_\bd may contain multi-\r\n",
      "       ple statements separated by newlines.  Leading whitespace  is  signifi-\r\n",
      "       cant  in  Python statements!  In non-interactive mode, the entire input\r\n",
      "       is parsed before it is executed.\r\n",
      "\r\n",
      "       If available, the script name and additional arguments  thereafter  are\r\n",
      "       passed  to  the script in the Python variable _\bs_\by_\bs_\b._\ba_\br_\bg_\bv, which is a list\r\n",
      "       of strings (you must first _\bi_\bm_\bp_\bo_\br_\bt _\bs_\by_\bs to be able to access it).  If  no\r\n",
      "       script  name  is  given, _\bs_\by_\bs_\b._\ba_\br_\bg_\bv_\b[_\b0_\b] is an empty string; if -\b-c\bc is used,\r\n",
      "       _\bs_\by_\bs_\b._\ba_\br_\bg_\bv_\b[_\b0_\b] contains the string _\b'_\b-_\bc_\b'_\b.  Note that options interpreted by\r\n",
      "       the Python interpreter itself are not placed in _\bs_\by_\bs_\b._\ba_\br_\bg_\bv.\r\n",
      "\r\n",
      "       In  interactive  mode,  the  primary prompt is `>>>'; the second prompt\r\n",
      "       (which appears when a command is not complete) is `...'.   The  prompts\r\n",
      "       can  be  changed  by assignment to _\bs_\by_\bs_\b._\bp_\bs_\b1 or _\bs_\by_\bs_\b._\bp_\bs_\b2.  The interpreter\r\n",
      "       quits when it reads an EOF at a prompt.  When  an  unhandled  exception\r\n",
      "       occurs,  a  stack  trace  is printed and control returns to the primary\r\n",
      "       prompt; in non-interactive mode, the interpreter exits  after  printing\r\n",
      "       the  stack  trace.   The  interrupt signal raises the _\bK_\be_\by_\bb_\bo_\ba_\br_\bd_\bI_\bn_\bt_\be_\br_\br_\bu_\bp_\bt\r\n",
      "       exception; other UNIX signals are not caught (except  that  SIGPIPE  is\r\n",
      "       sometimes  ignored, in favor of the _\bI_\bO_\bE_\br_\br_\bo_\br exception).  Error messages\r\n",
      "       are written to stderr.\r\n",
      "\r\n",
      "F\bFI\bIL\bLE\bES\bS A\bAN\bND\bD D\bDI\bIR\bRE\bEC\bCT\bTO\bOR\bRI\bIE\bES\bS\r\n",
      "       These are subject to difference depending on local installation conven-\r\n",
      "       tions;  ${prefix}  and  ${exec_prefix}  are  installation-dependent and\r\n",
      "       should be interpreted as for GNU software; they may be the  same.   The\r\n",
      "       default for both is _\b/_\bu_\bs_\br_\b/_\bl_\bo_\bc_\ba_\bl.\r\n",
      "\r\n",
      "       _\b$_\b{_\be_\bx_\be_\bc_\b__\bp_\br_\be_\bf_\bi_\bx_\b}_\b/_\bb_\bi_\bn_\b/_\bp_\by_\bt_\bh_\bo_\bn\r\n",
      "              Recommended location of the interpreter.\r\n",
      "\r\n",
      "       _\b$_\b{_\bp_\br_\be_\bf_\bi_\bx_\b}_\b/_\bl_\bi_\bb_\b/_\bp_\by_\bt_\bh_\bo_\bn_\b<_\bv_\be_\br_\bs_\bi_\bo_\bn_\b>\r\n",
      "       _\b$_\b{_\be_\bx_\be_\bc_\b__\bp_\br_\be_\bf_\bi_\bx_\b}_\b/_\bl_\bi_\bb_\b/_\bp_\by_\bt_\bh_\bo_\bn_\b<_\bv_\be_\br_\bs_\bi_\bo_\bn_\b>\r\n",
      "              Recommended locations of the directories containing the standard\r\n",
      "              modules.\r\n",
      "\r\n",
      "       _\b$_\b{_\bp_\br_\be_\bf_\bi_\bx_\b}_\b/_\bi_\bn_\bc_\bl_\bu_\bd_\be_\b/_\bp_\by_\bt_\bh_\bo_\bn_\b<_\bv_\be_\br_\bs_\bi_\bo_\bn_\b>\r\n",
      "       _\b$_\b{_\be_\bx_\be_\bc_\b__\bp_\br_\be_\bf_\bi_\bx_\b}_\b/_\bi_\bn_\bc_\bl_\bu_\bd_\be_\b/_\bp_\by_\bt_\bh_\bo_\bn_\b<_\bv_\be_\br_\bs_\bi_\bo_\bn_\b>\r\n",
      "              Recommended locations of the directories containing the  include\r\n",
      "              files  needed for developing Python extensions and embedding the\r\n",
      "              interpreter.\r\n",
      "\r\n",
      "       _\b~_\b/_\b._\bp_\by_\bt_\bh_\bo_\bn_\br_\bc_\b._\bp_\by\r\n",
      "              User-specific initialization file loaded by the _\bu_\bs_\be_\br module; not\r\n",
      "              used by default or by most applications.\r\n",
      "\r\n",
      "E\bEN\bNV\bVI\bIR\bRO\bON\bNM\bME\bEN\bNT\bT V\bVA\bAR\bRI\bIA\bAB\bBL\bLE\bES\bS\r\n",
      "       PYTHONHOME\r\n",
      "              Change  the  location  of  the  standard  Python  libraries.  By\r\n",
      "              default, the libraries are searched in ${prefix}/lib/python<ver-\r\n",
      "              sion>  and  ${exec_prefix}/lib/python<version>,  where ${prefix}\r\n",
      "              and ${exec_prefix} are installation-dependent directories,  both\r\n",
      "              defaulting  to  _\b/_\bu_\bs_\br_\b/_\bl_\bo_\bc_\ba_\bl.  When $PYTHONHOME is set to a single\r\n",
      "              directory, its value replaces both ${prefix} and ${exec_prefix}.\r\n",
      "              To specify different values for these, set $PYTHONHOME to ${pre-\r\n",
      "              fix}:${exec_prefix}.\r\n",
      "\r\n",
      "       PYTHONPATH\r\n",
      "              Augments the default search path for module files.   The  format\r\n",
      "              is  the  same  as the shell's $PATH: one or more directory path-\r\n",
      "              names  separated  by  colons.   Non-existent   directories   are\r\n",
      "              silently  ignored.   The  default  search  path  is installation\r\n",
      "              dependent, but generally begins  with  ${prefix}/lib/python<ver-\r\n",
      "              sion> (see PYTHONHOME above).  The default search path is always\r\n",
      "              appended to $PYTHONPATH.  If a script  argument  is  given,  the\r\n",
      "              directory containing the script is inserted in the path in front\r\n",
      "              of $PYTHONPATH.  The search path can be manipulated from  within\r\n",
      "              a Python program as the variable _\bs_\by_\bs_\b._\bp_\ba_\bt_\bh.\r\n",
      "\r\n",
      "       PYTHONSTARTUP\r\n",
      "              If  this  is the name of a readable file, the Python commands in\r\n",
      "              that file are executed before the first prompt is  displayed  in\r\n",
      "              interactive  mode.   The file is executed in the same name space\r\n",
      "              where interactive commands are executed so that objects  defined\r\n",
      "              or  imported  in  it  can  be  used without qualification in the\r\n",
      "              interactive session.  You can also change  the  prompts  _\bs_\by_\bs_\b._\bp_\bs_\b1\r\n",
      "              and _\bs_\by_\bs_\b._\bp_\bs_\b2 in this file.\r\n",
      "\r",
      "\r\n",
      "       PYTHONY2K\r\n",
      "              Set  this  to  a  non-empty  string  to cause the _\bt_\bi_\bm_\be module to\r\n",
      "              require dates specified as strings  to  include  4-digit  years,\r\n",
      "              otherwise  2-digit  years are converted based on rules described\r\n",
      "              in the _\bt_\bi_\bm_\be module documentation.\r\n",
      "\r\n",
      "       PYTHONOPTIMIZE\r\n",
      "              If this is set to a non-empty string it is equivalent to  speci-\r\n",
      "              fying  the  -\b-O\bO option. If set to an integer, it is equivalent to\r\n",
      "              specifying -\b-O\bO multiple times.\r\n",
      "\r\n",
      "       PYTHONDEBUG\r\n",
      "              If this is set to a non-empty string it is equivalent to  speci-\r\n",
      "              fying  the  -\b-d\bd option. If set to an integer, it is equivalent to\r\n",
      "              specifying -\b-d\bd multiple times.\r\n",
      "\r\n",
      "       PYTHONDONTWRITEBYTECODE\r\n",
      "              If this is set to a non-empty string it is equivalent to  speci-\r\n",
      "              fying the -\b-B\bB option (don't try to write _\b._\bp_\by_\b[_\bc_\bo_\b] files).\r\n",
      "\r\n",
      "       PYTHONINSPECT\r",
      "\r\n",
      "              If  this is set to a non-empty string it is equivalent to speci-\r\n",
      "              fying the -\b-i\bi option.\r\n",
      "\r\n",
      "       PYTHONIOENCODING\r\n",
      "              If this is set before running the interpreter, it overrides  the\r\n",
      "              encoding  used  for stdin/stdout/stderr, in the syntax _\be_\bn_\bc_\bo_\bd_\bi_\bn_\bg_\b-\r\n",
      "              _\bn_\ba_\bm_\be:\b:_\be_\br_\br_\bo_\br_\bh_\ba_\bn_\bd_\bl_\be_\br The _\be_\br_\br_\bo_\br_\bh_\ba_\bn_\bd_\bl_\be_\br part is optional and has  the\r\n",
      "              same meaning as in str.encode. For stderr, the _\be_\br_\br_\bo_\br_\bh_\ba_\bn_\bd_\bl_\be_\br\r\n",
      "               part is ignored; the handler will always be 'backslashreplace'.\r\n",
      "\r\n",
      "       PYTHONNOUSERSITE\r\n",
      "              If this is set to a non-empty string it is equivalent to  speci-\r\n",
      "              fying  the  -\b-s\bs  option  (Don't  add  the  user site directory to\r\n",
      "              sys.path).\r\n",
      "\r\n",
      "       PYTHONUNBUFFERED\r\n",
      "              If this is set to a non-empty string it is equivalent to  speci-\r\n",
      "              fying the -\b-u\bu option.\r\n",
      "\r\n",
      "       PYTHONVERBOSE\r\n",
      "              If  this is set to a non-empty string it is equivalent to speci-\r\n",
      "              fying the -\b-v\bv option. If set to an integer, it is  equivalent  to\r\n",
      "              specifying -\b-v\bv multiple times.\r\n",
      "\r\n",
      "       PYTHONWARNINGS\r\n",
      "              If  this  is set to a comma-separated string it is equivalent to\r\n",
      "              specifying the -\b-W\bW option for each separate value.\r\n",
      "\r\n",
      "       PYTHONHASHSEED\r\n",
      "              If this variable is set to \"random\", the effect is the  same  as\r\n",
      "              specifying  the  -\b-R\bR  option:  a random value is used to seed the\r\n",
      "              hashes of str, bytes and datetime objects.\r\n",
      "\r\n",
      "              If PYTHONHASHSEED is set to an integer value, it is  used  as  a\r\n",
      "              fixed seed for generating the hash() of the types covered by the\r\n",
      "              hash randomization.  Its purpose is to allow repeatable hashing,\r\n",
      "              such  as for selftests for the interpreter itself, or to allow a\r\n",
      "              cluster of python processes to share hash values.\r\n",
      "\r\n",
      "              The  integer  must  be   a   decimal   number   in   the   range\r\n",
      "              [0,4294967295].   Specifying  the  value 0 will lead to the same\r\n",
      "              hash values as when hash randomization is disabled.\r\n",
      "\r\n",
      "I\bIN\bNT\bTE\bER\bRA\bAC\bCT\bTI\bIV\bVE\bE I\bIN\bNP\bPU\bUT\bT E\bED\bDI\bIT\bTI\bIN\bNG\bG A\bAN\bND\bD H\bHI\bIS\bST\bTO\bOR\bRY\bY S\bSU\bUB\bBS\bST\bTI\bIT\bTU\bUT\bTI\bIO\bON\bN\r\n",
      "       The Python inteterpreter supports editing of the current input line and\r\n",
      "       history substitution, similar to facilities found in the Korn shell and\r\n",
      "       the GNU Bash shell.  However, rather than being implemented  using  the\r\n",
      "       GNU  Readline  library,  this  Python interpreter uses the BSD EditLine\r\n",
      "       library e\bed\bdi\bit\btl\bli\bin\bne\be(3) with a GNU Readline emulation layer.\r\n",
      "\r\n",
      "       The _\br_\be_\ba_\bd_\bl_\bi_\bn_\be module provides the access to the  EditLine  library,  but\r\n",
      "       there are a few major differences compared to a traditional implementa-\r\n",
      "       tion using the Readline library.  The  command  language  used  in  the\r\n",
      "       preference files is that of EditLine, as described in e\bed\bdi\bit\btr\brc\bc(5) and not\r\n",
      "       that  used  by  the  Readline  library.   This  also  means  that   the\r\n",
      "       _\bp_\ba_\br_\bs_\be_\b__\ba_\bn_\bd_\b__\bb_\bi_\bn_\bd()  routines  uses EditLine commands.  And the preference\r\n",
      "       file itself is _\b~_\b/_\b._\be_\bd_\bi_\bt_\br_\bc instead of _\b~_\b/_\b._\bi_\bn_\bp_\bu_\bt_\br_\bc.\r\n",
      "\r\n",
      "       For example, the _\br_\bl_\bc_\bo_\bm_\bp_\bl_\be_\bt_\be_\br module, which defines a  completion  func-\r\n",
      "       tion  for  the  _\br_\be_\ba_\bd_\bl_\bi_\bn_\be  modules,  works  correctly  with the EditLine\r\n",
      "       libraries, but needs to be initialized somewhat differently:\r\n",
      "\r\n",
      "              import rlcompleter\r\n",
      "              import readline\r\n",
      "              readline.parse_and_bind(\"bind ^I rl_complete\")\r\n",
      "\r\n",
      "       For _\bv_\bi mode, one needs:\r\n",
      "\r\n",
      "              readline.parse_and_bind(\"bind -v\")\r\n",
      "\r\n",
      "A\bAU\bUT\bTH\bHO\bOR\bR\r\n",
      "       The Python Software Foundation: https://www.python.org/psf/\r\n",
      "\r\n",
      "I\bIN\bNT\bTE\bER\bRN\bNE\bET\bT R\bRE\bES\bSO\bOU\bUR\bRC\bCE\bES\bS\r\n",
      "       Main website:  https://www.python.org/\r\n",
      "       Documentation:  https://docs.python.org/2/\r\n",
      "       Developer resources:  https://docs.python.org/devguide/\r\n",
      "       Downloads:  https://www.python.org/downloads/\r\n",
      "       Module repository:  https://pypi.python.org/\r\n",
      "       Newsgroups:  comp.lang.python, comp.lang.python.announce\r\n",
      "\r\n",
      "L\bLI\bIC\bCE\bEN\bNS\bSI\bIN\bNG\bG\r\n",
      "       Python is distributed under an  Open  Source  license.   See  the  file\r\n",
      "       \"LICENSE\"  in the Python source distribution for information on terms &\r\n",
      "       conditions for accessing and otherwise using  Python  and  for  a  DIS-\r\n",
      "       CLAIMER OF ALL WARRANTIES.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "                                                                     PYTHON(1)\r\n"
     ]
    }
   ],
   "source": [
    "!man python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: hdfs: command not found\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим  `wordcount` директорию с вложенной директорий `input` в Hadoop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bash: line 2: hdfs: command not found\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# mkdir - создание новых каталогов\n",
    "hdfs dfs -mkdir -p wordcount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скопируем fruits.txt в Hadoop директории `wordcount/input`.\n",
    "\n",
    "Почему мы это делаем? Файл `fruits.txt` должен располагаться в файловой системе Hadoop, а не локально. Когда файл находится в файловой системе Hadoop, то мы получаем возможность использовать фишки Hadoop: data partitioning, distributed processing, fault tolerance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bash: line 4: hdfs: command not found\n",
      "bash: line 5: hdfs: command not found\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# 2> file redirects stderr to file\n",
    "# /dev/null is the null device it takes any input you want and throws it away. It can be used to suppress any output.\n",
    "hdfs dfs -rm -r wordcount/input 2>/dev/null\n",
    "hdfs dfs -mkdir wordcount/input\n",
    "hdfs dfs -put fruits.txt wordcount/input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь проверим.\n",
    "\n",
    "**Note:** Используйте опцию `-h` для `ls`, чтобы показать размер файла в `human-readable` форме"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: hdfs: command not found\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls -h -R wordcount/input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тестируем MapReduce на простом reducer <a name=\"dummyreducer\"></a>\n",
    "\n",
    "Попроюуем запустить наш mapper используя простой reducer `/bin/cat`.\n",
    "\n",
    "**Warning:** mapreduce процесс всегда выводит большие output в командную строку, часто эта информация для нас бесполезна, нам нужен будет этот результат: <html><pre>\"INFO mapreduce.Job: Job ... completed successfully\"</pre></html>\n",
    "\n",
    "**Note:** Когда вы запускаете процесс, убедитесь, что финального файла нет в системе, иначе вы получите ошибку. Вы можете всегда добавлять такое действие: `hadoop fs -rmr wordcount/output 2>/dev/null`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "\n",
    "# Это из файла hadoop-spark-standalone-docker/hadoop-standalone/submit/run.sh \n",
    "\n",
    "# STREAMING инициализирует map-reduce\n",
    "STREAMING=$HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.2.1.jar\n",
    "\n",
    "\n",
    "hdfs dfs -rm -r /output 2> /dev/null\n",
    "\n",
    "# $HADOOP_HOME/bin/hadoop - это путь к хадупу\n",
    "# jar - указываем класс процесса\n",
    "# $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.2.1.jar - тот самый класс,\n",
    "# которй инициализирует mapreduce процесс в хадупе (Hadoop streaming)\n",
    "$HADOOP_HOME/bin/hadoop jar $STREAMING\\\n",
    "    -input /input/*\\\n",
    "    # это ключ-значение: значение - пустое место, ключ - само слово\n",
    "    # tr заменяет каждый пробел на \\n\n",
    "    -mapper 'tr \" \" \"\\n\"' \\\n",
    "    -output /output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-15-ae169633e549>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-ae169633e549>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    %%bash\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# здес все то же самое, что и в предыдущей строке\n",
    "%%bash\n",
    "hdfs dfs -rm -r wordcount/output 2>/dev/null\n",
    "mapred streaming \\\n",
    "  -files map.sh \\\n",
    "  -input wordcount/input \\\n",
    "  -output wordcount/output \\\n",
    "  -mapper map.sh \\\n",
    "  -reducer /bin/cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   3 datalab supergroup          0 2019-11-18 08:50 wordcount/output/_SUCCESS\n",
      "-rw-r--r--   3 datalab supergroup         78 2019-11-18 08:50 wordcount/output/part-00000\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls wordcount/output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если `output` содержит файл `_SUCCESS`, то ваш процесс завершился удачно\n",
    "\n",
    "**Note:** когда работает с большими данными, то к `cat` используйте `head` или `tail`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: hdfs: command not found\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat wordcount/output/part*|head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffling and sorting <a name=\"shuffling&sorting\"></a>\n",
    "\n",
    "На изображении представлен процесс по результатам 2х мапперов. Процессы shuffle и sort происходят до попадания результатов в reducer\n",
    "\n",
    "![shuffle & sort](shuffle_sort.png)\n",
    "\n",
    "Shuffling и sorting являются самыми \"дорогими\" процессами в MapReduce.\n",
    "\n",
    "\n",
    "<b>Note:</b>  $2$ - базовое количество мапперов в Hadoop. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducer <a name=\"reducer\"></a>\n",
    "Напишем скрипт reducer `reduce.sh`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing reduce.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile reduce.sh\n",
    "#!/bin/bash\n",
    "\n",
    "currkey=\"\"\n",
    "currcount=0\n",
    "while IFS=$'\\t' read -r key val\n",
    "do\n",
    "  if [[ $key == $currkey ]]\n",
    "  then\n",
    "      currcount=$(( currcount + val ))\n",
    "  else\n",
    "    if [ -n \"$currkey\" ]\n",
    "    then\n",
    "      echo -e ${currkey} \"\\t\" ${currcount} \n",
    "    fi\n",
    "    currkey=$key\n",
    "    currcount=1\n",
    "  fi\n",
    "done\n",
    "\n",
    "# stdout \n",
    "echo -e ${currkey} \"\\t\" ${currcount}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Установим правила для нашего reducer скрипта"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod 700 reduce.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Запускаем reducer <a name=\"run\"></a>\n",
    "\n",
    "Выполним map и reduce без hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple \t 2\n",
      "banana \t 1\n",
      "orange \t 1\n",
      "peach \t 4\n",
      "pineapple \t 1\n"
     ]
    }
   ],
   "source": [
    "!cat fruits.txt|./map.sh|sort|./reduce.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это хороший способ тестирования, если результаты правильные, то запустим на Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [map.sh, reduce.sh] [/opt/cloudera/parcels/CDH-6.3.0-1.cdh6.3.0.p0.1279813/jars/hadoop-streaming-3.0.0-cdh6.3.0.jar] /tmp/streamjob4511756596880012363.jar tmpDir=null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19/11/18 08:50:23 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "19/11/18 08:50:25 INFO client.ConfiguredRMFailoverProxyProvider: Failing over to rm472\n",
      "19/11/18 08:50:25 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /user/datalab/.staging/job_1571899345744_0252\n",
      "19/11/18 08:50:26 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "19/11/18 08:50:26 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "19/11/18 08:50:26 INFO Configuration.deprecation: yarn.resourcemanager.zk-address is deprecated. Instead, use hadoop.zk.address\n",
      "19/11/18 08:50:26 INFO Configuration.deprecation: yarn.resourcemanager.system-metrics-publisher.enabled is deprecated. Instead, use yarn.system-metrics-publisher.enabled\n",
      "19/11/18 08:50:26 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1571899345744_0252\n",
      "19/11/18 08:50:26 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "19/11/18 08:50:26 INFO conf.Configuration: resource-types.xml not found\n",
      "19/11/18 08:50:26 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "19/11/18 08:50:26 INFO impl.YarnClientImpl: Submitted application application_1571899345744_0252\n",
      "19/11/18 08:50:26 INFO mapreduce.Job: The url to track the job: http://c101.local:8088/proxy/application_1571899345744_0252/\n",
      "19/11/18 08:50:26 INFO mapreduce.Job: Running job: job_1571899345744_0252\n",
      "19/11/18 08:50:36 INFO mapreduce.Job: Job job_1571899345744_0252 running in uber mode : false\n",
      "19/11/18 08:50:36 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "19/11/18 08:50:43 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "19/11/18 08:50:49 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "19/11/18 08:50:50 INFO mapreduce.Job: Job job_1571899345744_0252 completed successfully\n",
      "19/11/18 08:50:50 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=79\n",
      "\t\tFILE: Number of bytes written=687864\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=312\n",
      "\t\tHDFS: Number of bytes written=56\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=484692\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=241904\n",
      "\t\tTotal time spent by all map tasks (ms)=9321\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4652\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=9321\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=4652\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=47723520\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=23818240\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=3\n",
      "\t\tMap output records=9\n",
      "\t\tMap output bytes=78\n",
      "\t\tMap output materialized bytes=108\n",
      "\t\tInput split bytes=222\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=5\n",
      "\t\tReduce shuffle bytes=108\n",
      "\t\tReduce input records=9\n",
      "\t\tReduce output records=5\n",
      "\t\tSpilled Records=18\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=158\n",
      "\t\tCPU time spent (ms)=2550\n",
      "\t\tPhysical memory (bytes) snapshot=1526378496\n",
      "\t\tVirtual memory (bytes) snapshot=18883096576\n",
      "\t\tTotal committed heap usage (bytes)=4490002432\n",
      "\t\tPeak Map Physical memory (bytes)=580694016\n",
      "\t\tPeak Map Virtual memory (bytes)=6292299776\n",
      "\t\tPeak Reduce Physical memory (bytes)=368877568\n",
      "\t\tPeak Reduce Virtual memory (bytes)=6298894336\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=90\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=56\n",
      "19/11/18 08:50:50 INFO streaming.StreamJob: Output directory: wordcount/output\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "hdfs dfs -rm -r wordcount/output 2>/dev/null\n",
    "mapred streaming \\\n",
    "  -file map.sh \\\n",
    "  -file reduce.sh \\\n",
    "  -input wordcount/input \\\n",
    "  -output wordcount/output \\\n",
    "  -mapper map.sh \\\n",
    "  -reducer reduce.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим файл результата в HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple \t 2\n",
      "banana \t 1\n",
      "orange \t 1\n",
      "peach \t 4\n",
      "pineapple \t 1\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat wordcount/output/part*|head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Запускаем mapreduce job с большими данными <a name=\"moredata\"></a>\n",
    "\n",
    "Создадим файл с данными, на основе реальной новостой статьи из интернета. Данных стало больше, представим, что это \"большие данные\".\n",
    "\n",
    "Для загрузки данных из интернета используйте, либо парсер Python, либо команду `wget` и удалите HTML командой `sed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "URL=https://www.derstandard.at/story/2000110819049/und-wo-warst-du-beim-fall-der-mauer\n",
    "wget -qO- $URL | sed -e 's/<[^>]*>//g;s/^ //g' >sample_article.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0% [                                                                              ]     0 / 17297\r",
      " 47% [....................................                                          ]  8192 / 17297\r",
      " 94% [.........................................................................     ] 16384 / 17297\r",
      "100% [..............................................................................] 17297 / 17297"
     ]
    }
   ],
   "source": [
    "# wget на Python\n",
    "import wget\n",
    "import lxml.html\n",
    "\n",
    "url = 'https://www.derstandard.at/story/2000110819049/und-wo-warst-du-beim-fall-der-mauer'\n",
    "filename = wget.download(url)\n",
    "\n",
    "with open(filename, 'r') as f:\n",
    "    ff = f.read()\n",
    "    t = lxml.html.fromstring(ff)\n",
    "    result = t.text_content()\n",
    "    \n",
    "with open('sample_article.txt', 'w+', encoding='utf-8') as f:\n",
    "    f.write(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1\n",
      "\t1\n",
      "\t1\n",
      "\t1\n",
      "Und\t1\n",
      "wo\t1\n",
      "warst\t1\n",
      "du\t1\n",
      "beim\t1\n",
      "Fall\t1\n"
     ]
    }
   ],
   "source": [
    "!cat sample_article.txt|./map.sh|head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изменим наш маппер для работы с пустыми строками."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting map.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile map.sh\n",
    "#!/bin/bash\n",
    "\n",
    "while read line\n",
    "do\n",
    " for word in $line\n",
    " do\n",
    "  if [[ \"$line\" =~ [^[:space:]] ]]\n",
    "  then\n",
    "    if [ -n \"$word\" ]\n",
    "    then\n",
    "    echo -e ${word} \"\\t1\"\n",
    "    fi\n",
    "  fi\n",
    " done\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Und \t1\n",
      "wo \t1\n",
      "warst \t1\n",
      "du \t1\n",
      "beim \t1\n",
      "Fall \t1\n",
      "der \t1\n",
      "Mauer? \t1\n",
      "- \t1\n",
      " \t1\n"
     ]
    }
   ],
   "source": [
    "!cat sample_article.txt|./map.sh|head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`map.sh` дает лучше результаты\n",
    "\n",
    "<b>Note:</b> при работе с реальными данными мы должны обращать внимание на их \"чистоту\" и добавлять в свой код процессы обработки (отчистки) данных.\n",
    "\n",
    "Запустим MapReduce на новых данных, но сначала загрузим их в HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -rm -r wordcount/input 2>/dev/null\n",
    "hdfs dfs -put sample_article.txt wordcount/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   3 datalab supergroup      4.1 K 2019-11-18 08:50 wordcount/input\n"
     ]
    }
   ],
   "source": [
    "# проверим\n",
    "!hdfs dfs -ls -h wordcount/input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "проверим reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Und \t 1\n",
      "wo \t 1\n",
      "warst \t 1\n",
      "du \t 1\n",
      "beim \t 1\n",
      "Fall \t 1\n",
      "der \t 1\n",
      "Mauer? \t 1\n",
      "- \t 1\n",
      " \t 2\n"
     ]
    }
   ],
   "source": [
    "!cat sample_article.txt|./map.sh|./reduce.sh|head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "hadoop fs -rmr wordcount/output 2>/dev/null\n",
    "mapred streaming \\\n",
    "  -file map.sh \\\n",
    "  -file reduce.sh \\\n",
    "  -input wordcount/input \\\n",
    "  -output wordcount/output \\\n",
    "  -mapper map.sh \\\n",
    "  -reducer reduce.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим результат в HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   3 datalab supergroup          0 2019-11-18 08:51 wordcount/output/_SUCCESS\n",
      "-rw-r--r--   3 datalab supergroup       2273 2019-11-18 08:51 wordcount/output/part-00000\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls wordcount/output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Больше данных - больше времени, так что этот процесс. Но учтите, что не всегда стоит использовать Hadoop, есть много кейсов, когда не стоит использовать сложные инструменты. Hadoop - только для больших инструментов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&amp; \t 1\n",
      "(Herder-Verlag) \t 1\n",
      "- \t 1\n",
      "/ \t 2\n",
      "1950 \t 1\n",
      "24 \t 1\n",
      "30 \t 1\n",
      "30-Jährige \t 1\n",
      "<path \t 1\n",
      "AGB \t 1\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat wordcount/output/part-00000|head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort результат (`sort`) <a name=\"sortoutput\"></a>\n",
    "\n",
    "В результате мы получаем список из связки: слово - значение, нам нужно сделать сортировку по частоте употребления слова.\n",
    "\n",
    "Результат из reducer сортируется по ключам (словам) на основе результатов из mapper. Для получения сортированного результата используем Unix команду `sort` (с опциями `k2`, `n`, `r`, которое означают \"по полю 2\", \"числовое значение\", \"от большего к меньшему\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "die \t 8\n",
      "der \t 6\n",
      "Cookies \t 4\n",
      "und \t 4\n",
      "derStandard.at \t 3\n",
      "Fall \t 3\n",
      "ich \t 3\n",
      "in \t 3\n",
      "kann \t 3\n",
      "ohne \t 3\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat wordcount/output/part-00000|sort -k2nr|head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort результат (в MapReduce) <a name=\"sortoutputMR\"></a>\n",
    "\n",
    "Если нам необходимо сделать сортировку в reducer, то мы можем применить простой трюк: создаем маппер, который будет менять местами слова (ключи) и их частоту (значение), на выходе маппера мы получим желанный эфект автоматически.\n",
    "\n",
    "Создадим новый маппер `swap_keyval.sh`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing swap_keyval.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile swap_keyval.sh\n",
    "#!/bin/bash\n",
    "# скрипт меняет местами значения в строке\n",
    "# пример: \"word 100\" -> \"100 word\"\n",
    "\n",
    "while read key val\n",
    "do\n",
    " printf \"%s\\t%s\\n\" \"$val\" \"$key\"\n",
    "done    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выполним наш новый маппер в старом пайплайне, не забудьте удалить `output_sorted`. \n",
    "\n",
    "Каждый шаг записывает свой результат на диск, что при больших объемах увеличивает время выполнения, это одна из причин появления Apache Spark [Apache Spark](https://spark.apache.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -rm -r wordcount/output2 2>/dev/null\n",
    "mapred streaming \\\n",
    "  -file swap_keyval.sh \\\n",
    "  -input wordcount/output \\\n",
    "  -output wordcount/output2 \\\n",
    "  -mapper swap_keyval.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим результат в HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: hdfs: command not found\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls wordcount/output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\tan\n",
      "1\t–\n",
      "1\tüberraschen.\n",
      "1\tÜber\n",
      "1\tzustimmungspflichtige\n",
      "1\tzustimmen\n",
      "1\tzum\n",
      "1\tzu.\n",
      "1\twiderrufen.\n",
      "1\twerden.\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat wordcount/output2/part-00000|head "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Маппер сортирует от меньшего к большему (sort = ascending order). Самые частые слова будут внизу файла (смотрим хвост файла)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\tkann\n",
      "3\tin\n",
      "3\tderStandard.at\n",
      "3\tich\n",
      "3\tFall\n",
      "3\tSie\n",
      "4\tCookies\n",
      "4\tund\n",
      "6\tder\n",
      "8\tdie\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat wordcount/output2/part-00000|tail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Конфигурируем сортировку с `KeyFieldBasedComparator` <a name=\"KeyFieldBasedComparator\"></a>\n",
    "\n",
    "Мы можем определить, как маппер будет сортировать свои результаты, для этого надо исользовать класс [`KeyFieldBasedComparator`](https://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapreduce/lib/partition/KeyFieldBasedComparator.html)\n",
    "<html><pre>-D mapreduce.job.output.key.comparator.class=\\\n",
    "    org.apache.hadoop.mapred.lib.KeyFieldBasedComparator</pre></html>\n",
    "    \n",
    "Данный класс (из библиотеке Hadoop) позволяет сделать похожие дополнения (опции) к сортировке, как Unix `sort`(`-n` - чиловая сортировка, `-r` от большего к меньшему, `-k pos1[,pos2]` сортировать по позиции элемента).\n",
    "\n",
    "Применим данный класс `KeyFieldBasedComparator` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -rmr wordcount/output2 2>/dev/null\n",
    "comparator_class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator\n",
    "mapred streaming \\\n",
    "  -D mapreduce.job.output.key.comparator.class=$comparator_class \\\n",
    "  -D mapreduce.partition.keycomparator.options=-nr \\\n",
    "  -file swap_keyval.sh \\\n",
    "  -input wordcount/output \\\n",
    "  -output wordcount/output2 \\\n",
    "  -mapper swap_keyval.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   3 datalab supergroup          0 2019-11-18 08:52 wordcount/output2/_SUCCESS\n",
      "-rw-r--r--   3 datalab supergroup       1945 2019-11-18 08:52 wordcount/output2/part-00000\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls wordcount/output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\tdie\n",
      "6\tder\n",
      "4\tund\n",
      "4\tCookies\n",
      "3\tFall\n",
      "3\tSie\n",
      "3\tich\n",
      "3\tkann\n",
      "3\twarst\n",
      "3\tin\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat wordcount/output2/part-00000|head "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы получили необходимый результат"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Определяем конфигурацию опцией -D<a name=\"configuration_variables\"></a>\n",
    "\n",
    "Опция `-D` позволяет перезаписать параметр в базовой конфигурации [`mapred_default.xml`](https://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml)\n",
    "(в документации [Apache Hadoop documentation](https://hadoop.apache.org/docs/current/hadoop-streaming/HadoopStreaming.html#Specifying_Configuration_Variables_with_the_-D_Option)).\n",
    "\n",
    "Иногда это требуется для исправления ошибки `out-of-memory` во время сортировки. Так как выделенной памяти может не хватать и её нужно увеличить, а если процесс выполняется и ресурсов очень много, то уменьшить. За это отвечает параметр `mapreduce.task.io.sort.mb`, он имеет размерность в Mb:\n",
    " <html>\n",
    "    <pre>-D mapreduce.task.io.sort.mb=512\n",
    "    </pre>\n",
    " </html>\n",
    "\n",
    " **Note:** `mapreduce.task.io.sort.mb` может быть не более  2047.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
